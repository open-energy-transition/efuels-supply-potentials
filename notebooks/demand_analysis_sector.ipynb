{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniforge3/envs/pypsa-earth/lib/python3.10/site-packages/pypsa/networkclustering.py:16: UserWarning:\n",
      "\n",
      "The namespace `pypsa.networkclustering` is deprecated and will be removed in PyPSA v0.24. Please use `pypsa.clustering.spatial instead`. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pypsa\n",
    "import pycountry\n",
    "import pandas as pd\n",
    "os.chdir(\"/home/user/Desktop/network_analysis\")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import geopandas as gpd\n",
    "import re\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pypsa.io:Imported network elec_s_10_ec_lcopt_Co2L-24H.nc has buses, carriers, generators, global_constraints, lines, links, loads, storage_units, stores\n",
      "INFO:pypsa.io:Imported network elec_s_10_ec_lcopt_Co2L-24H_24H_2020_0.071_AB_10export.nc has buses, carriers, generators, global_constraints, lines, links, loads, storage_units, stores\n",
      "INFO:pypsa.io:Imported network elec_s_10_ec_lcopt_24H.nc has buses, carriers, generators, lines, links, loads, storage_units, stores\n",
      "INFO:pypsa.io:Imported network elec_s_10_ec_lcopt_24H_24H_2030_0.071_AB_10export.nc has buses, carriers, generators, lines, links, loads, storage_units, stores\n",
      "INFO:pypsa.io:Imported network elec_s_10_ec_lcopt_24H.nc has buses, carriers, generators, lines, links, loads, storage_units, stores\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/user/Desktop/network_analysis/US_2040/Medium/postnetworks/elec_s_100_ec_lcopt_24H_24H_2040_0.071_AB_10export.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/pypsa-earth/lib/python3.10/site-packages/xarray/backends/file_manager.py:211\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/pypsa-earth/lib/python3.10/site-packages/xarray/backends/lru_cache.py:56\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 56\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mmove_to_end(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('/home/user/Desktop/network_analysis/US_2040/Medium/postnetworks/elec_s_100_ec_lcopt_24H_24H_2040_0.071_AB_10export.nc',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), 'a696f97a-85af-47dd-b023-6a65f407900a']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m networks \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, path_i \u001b[38;5;129;01min\u001b[39;00m n_names\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 21\u001b[0m     networks[name] \u001b[38;5;241m=\u001b[39m \u001b[43mpypsa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_i\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/pypsa-earth/lib/python3.10/site-packages/pypsa/components.py:343\u001b[0m, in \u001b[0;36mNetwork.__init__\u001b[0;34m(self, import_name, name, ignore_standard_types, override_components, override_component_attrs, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimport_from_hdf5(import_name)\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(import_name)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 343\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_from_netcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimport_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m import_name\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimport_from_csv_folder(import_name)\n",
      "File \u001b[0;32m~/miniforge3/envs/pypsa-earth/lib/python3.10/site-packages/pypsa/io.py:653\u001b[0m, in \u001b[0;36mimport_from_netcdf\u001b[0;34m(network, path, skip_time)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m has_xarray, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxarray must be installed for netCDF support.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    652\u001b[0m basename \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m--> 653\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mImporterNetCDF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m importer:\n\u001b[1;32m    654\u001b[0m     _import_from_importer(network, importer, basename\u001b[38;5;241m=\u001b[39mbasename, skip_time\u001b[38;5;241m=\u001b[39mskip_time)\n",
      "File \u001b[0;32m~/miniforge3/envs/pypsa-earth/lib/python3.10/site-packages/pypsa/io.py:291\u001b[0m, in \u001b[0;36mImporterNetCDF.__init__\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m validators\u001b[38;5;241m.\u001b[39murl(\u001b[38;5;28mstr\u001b[39m(path)):\n\u001b[1;32m    290\u001b[0m         path \u001b[38;5;241m=\u001b[39m _retrieve_from_url(path)\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds \u001b[38;5;241m=\u001b[39m path\n",
      "File \u001b[0;32m~/miniforge3/envs/pypsa-earth/lib/python3.10/site-packages/xarray/backends/api.py:572\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    561\u001b[0m     decode_cf,\n\u001b[1;32m    562\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    568\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    569\u001b[0m )\n\u001b[1;32m    571\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 572\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    578\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    579\u001b[0m     backend_ds,\n\u001b[1;32m    580\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    591\u001b[0m )\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/miniforge3/envs/pypsa-earth/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:593\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, lock, autoclose)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mopen_dataset\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]  # allow LSP violation, not supporting **kwargs\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    574\u001b[0m     filename_or_obj: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m os\u001b[38;5;241m.\u001b[39mPathLike[Any] \u001b[38;5;241m|\u001b[39m BufferedIOBase \u001b[38;5;241m|\u001b[39m AbstractDataStore,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    590\u001b[0m     autoclose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    591\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m    592\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[0;32m--> 593\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[43mNetCDF4DataStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclobber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclobber\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiskless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiskless\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    605\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    606\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[0;32m~/miniforge3/envs/pypsa-earth/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:400\u001b[0m, in \u001b[0;36mNetCDF4DataStore.open\u001b[0;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001b[0m\n\u001b[1;32m    394\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    395\u001b[0m     clobber\u001b[38;5;241m=\u001b[39mclobber, diskless\u001b[38;5;241m=\u001b[39mdiskless, persist\u001b[38;5;241m=\u001b[39mpersist, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m\n\u001b[1;32m    396\u001b[0m )\n\u001b[1;32m    397\u001b[0m manager \u001b[38;5;241m=\u001b[39m CachingFileManager(\n\u001b[1;32m    398\u001b[0m     netCDF4\u001b[38;5;241m.\u001b[39mDataset, filename, mode\u001b[38;5;241m=\u001b[39mmode, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    399\u001b[0m )\n\u001b[0;32m--> 400\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/pypsa-earth/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:347\u001b[0m, in \u001b[0;36mNetCDF4DataStore.__init__\u001b[0;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group \u001b[38;5;241m=\u001b[39m group\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m--> 347\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[38;5;241m.\u001b[39mdata_model\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mfilepath()\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_remote \u001b[38;5;241m=\u001b[39m is_remote_uri(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename)\n",
      "File \u001b[0;32m~/miniforge3/envs/pypsa-earth/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:409\u001b[0m, in \u001b[0;36mNetCDF4DataStore.ds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 409\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/pypsa-earth/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:403\u001b[0m, in \u001b[0;36mNetCDF4DataStore._acquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39macquire_context(needs_lock) \u001b[38;5;28;01mas\u001b[39;00m root:\n\u001b[1;32m    404\u001b[0m         ds \u001b[38;5;241m=\u001b[39m _nc4_require_group(root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode)\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/miniforge3/envs/pypsa-earth/lib/python3.10/contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/pypsa-earth/lib/python3.10/site-packages/xarray/backends/file_manager.py:199\u001b[0m, in \u001b[0;36mCachingFileManager.acquire_context\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    198\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m     file, cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire_with_cache_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
      "File \u001b[0;32m~/miniforge3/envs/pypsa-earth/lib/python3.10/site-packages/xarray/backends/file_manager.py:217\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    215\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    216\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\n\u001b[0;32m--> 217\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opener\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2521\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2158\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/user/Desktop/network_analysis/US_2040/Medium/postnetworks/elec_s_100_ec_lcopt_24H_24H_2040_0.071_AB_10export.nc'"
     ]
    }
   ],
   "source": [
    "path_2023 = \"US_2023/networks/elec_s_10_ec_lcopt_Co2L-24H.nc\"\n",
    "path_2023_sec = \"US_2023/postnetworks/elec_s_10_ec_lcopt_Co2L-24H_24H_2020_0.071_AB_10export.nc\"\n",
    "\n",
    "path_2030 = \"US_2030/Medium/networks/elec_s_10_ec_lcopt_24H.nc\"\n",
    "path_2030_sec = \"US_2030/Medium/postnetworks/elec_s_10_ec_lcopt_24H_24H_2030_0.071_AB_10export.nc\"\n",
    "\n",
    "path_2040 = \"US_2040/Medium/networks/elec_s_10_ec_lcopt_24H.nc\"\n",
    "path_2040_sec = \"US_2040/Medium/postnetworks/elec_s_100_ec_lcopt_24H_24H_2040_0.071_AB_10export.nc\"\n",
    "\n",
    "n_names = {\n",
    "    \"2023 power\": path_2023,\n",
    "    \"2023 sector\": path_2023_sec,\n",
    "    \"2030 power\": path_2030,\n",
    "    \"2030 sector\": path_2030_sec,\n",
    "    \"2040 power\": path_2040,\n",
    "    \"2040 sector\": path_2040_sec,\n",
    "}\n",
    "\n",
    "networks = {}\n",
    "for name, path_i in n_names.items():\n",
    "    networks[name] = pypsa.Network(path_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_processes = [\n",
    "    \"SMR CC\", \"Haber-Bosch\", \"ethanol from starch\", \"ethanol from starch CC\",\n",
    "    \"DRI\", \"DRI CC\", \"DRI H2\", \"BF-BOF\", \"BF-BOF CC\", \"EAF\",\n",
    "    \"dry clinker\", \"cement finishing\", \"dry clinker CC\"\n",
    "]\n",
    "for name, n in networks.items():\n",
    "    if \"sector\" not in name:\n",
    "        continue\n",
    "\n",
    "    nhours = n.snapshot_weightings.objective.sum()\n",
    "\n",
    "    # 1. Look for all links related to target_processes\n",
    "    process_links = n.links[n.links.carrier.isin(target_processes)]\n",
    "\n",
    "    # 2. Filter AC input links\n",
    "    ac_input_links = process_links[process_links.bus0.map(n.buses.carrier) == \"AC\"].index\n",
    "\n",
    "    # 3. Sum AC consumption\n",
    "    ac_industrial_profile = n.links_t.p0[ac_input_links].sum(axis=1)\n",
    "    ac_industrial_twh = (ac_industrial_profile * n.snapshot_weightings.objective).sum() / 1e6\n",
    "\n",
    "    print(f\"{name} → Industrial AC: {ac_industrial_twh:.2f} TWh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_processes = [\n",
    "    \"SMR CC\", \"Haber-Bosch\", \"ethanol from starch\", \"ethanol from starch CC\",\n",
    "    \"DRI\", \"DRI CC\", \"DRI H2\", \"BF-BOF\", \"BF-BOF CC\", \"EAF\",\n",
    "    \"dry clinker\", \"cement finishing\", \"dry clinker CC\"\n",
    "]\n",
    "\n",
    "static_load_carriers = [\"rail transport electricity\", \"agriculture electricity\", \"industry electricity\"]\n",
    "dynamic_load_carriers = [\"AC\", \"services electricity\", \"land transport EV\"]\n",
    "demand = pd.DataFrame(columns=networks.keys(), index=dynamic_load_carriers+static_load_carriers+[\"total demand\"])\n",
    "\n",
    "for name, n in networks.items():\n",
    "    if \"sector\" not in name:\n",
    "        continue\n",
    "\n",
    "    nhours = n.snapshot_weightings.objective.sum()\n",
    "\n",
    "    # 1. Static loads (TWh)\n",
    "    static_totals = (\n",
    "        n.loads.groupby(\"carrier\")\n",
    "               .sum().p_set\n",
    "               .reindex(static_load_carriers)\n",
    "               .fillna(0)\n",
    "    )\n",
    "    static_load_twh = static_totals.sum() * nhours / 1e6\n",
    "\n",
    "    # 2. Separate industrial and non-industrial AC\n",
    "    process_links = n.links[n.links.carrier.isin(target_processes)]\n",
    "    ac_input_links = process_links[process_links.bus0.map(n.buses.carrier) == \"AC\"].index\n",
    "\n",
    "    ind_ac_profile = n.links_t.p0[ac_input_links].sum(axis=1)\n",
    "    ind_ac_twh = (ind_ac_profile * n.snapshot_weightings.objective).sum() / 1e6\n",
    "\n",
    "    ac_loads = n.loads[n.loads.carrier == \"AC\"]\n",
    "    industrial_ac_buses = n.links.loc[ac_input_links, \"bus0\"].unique()\n",
    "    ac_non_ind_idx = ac_loads[~ac_loads.bus.isin(industrial_ac_buses)].index\n",
    "    ac_profile = n.loads_t.p_set[ac_non_ind_idx.intersection(n.loads_t.p_set.columns)].sum(axis=1)\n",
    "    ac_twh = (ac_profile * n.snapshot_weightings.objective).sum() / 1e6 - ind_ac_twh\n",
    "\n",
    "    # 3. Services & EV\n",
    "    serv_idx = [i for i in n.loads[n.loads.carrier == \"services electricity\"].index\n",
    "                if i in n.loads_t.p_set.columns]\n",
    "    ev_idx   = [i for i in n.loads[n.loads.carrier == \"land transport EV\"].index\n",
    "                if i in n.loads_t.p_set.columns]\n",
    "\n",
    "    serv_profile = n.loads_t.p_set[serv_idx].sum(axis=1) if serv_idx else 0\n",
    "    ev_profile   = n.loads_t.p_set[ev_idx].sum(axis=1)   if ev_idx else 0\n",
    "\n",
    "    serv_twh = (serv_profile * n.snapshot_weightings.objective).sum() / 1e6\n",
    "    ev_twh   = (ev_profile   * n.snapshot_weightings.objective).sum() / 1e6\n",
    "\n",
    "    # 4. Industry electricity total = static + industrial AC\n",
    "    industry_static_twh = static_totals.get(\"industry electricity\", 0) * nhours / 1e6\n",
    "    industry_elec_twh = industry_static_twh + ind_ac_twh\n",
    "\n",
    "    demand.loc[\"rail transport electricity\", name] = static_totals.get(\"rail transport electricity\", 0) * nhours / 1e6\n",
    "    demand.loc[\"agriculture electricity\",      name] = static_totals.get(\"agriculture electricity\",  0) * nhours / 1e6\n",
    "    demand.loc[\"industry electricity\",         name] = industry_elec_twh\n",
    "    demand.loc[\"AC\",                           name] = ac_twh\n",
    "    demand.loc[\"services electricity\",         name] = serv_twh\n",
    "    demand.loc[\"land transport EV\",            name] = ev_twh\n",
    "    demand.loc[\"total demand\",                 name] = (\n",
    "        static_load_twh + ac_twh + ind_ac_twh + serv_twh + ev_twh\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrel_data = {\n",
    "    \"AC\":                    [1360.81, 1359.91, 1392.19],\n",
    "    \"services electricity\":  [1381.11, 1398.69, 1579.71],\n",
    "    \"land transport EV\":     [13.10,   73.95,   59.70],\n",
    "    \"rail transport electricity\": [7.29, 7.64, 8.29],\n",
    "    \"agriculture electricity\": [0, 0, 0],\n",
    "    \"industry electricity\":  [1036.81, 1109.69, 1213.94],\n",
    "    \"total demand\": [3799.12, 4293.30, 5072.86],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "years = [\"2023\", \"2030\", \"2040\"]\n",
    "subcols = [\"power\", \"sector\", \"NREL\", \"diff %\"]\n",
    "\n",
    "multi_cols = pd.MultiIndex.from_product([years, subcols])\n",
    "new_demand = pd.DataFrame(index=demand.index, columns=multi_cols)\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    new_demand[(year, \"power\")]  = demand[f\"{year} power\"]\n",
    "    new_demand[(year, \"sector\")] = demand[f\"{year} sector\"]\n",
    "\n",
    "    for idx in demand.index:\n",
    "        nrel_val = nrel_data.get(idx, [None] * 3)[i]\n",
    "        new_demand.at[idx, (year, \"NREL\")] = nrel_val\n",
    "\n",
    "        if nrel_val not in [None, 0]:\n",
    "            sector_val = demand.at[idx, f\"{year} sector\"]\n",
    "            new_demand.at[idx, (year, \"diff %\")] = 100 * (sector_val - nrel_val) / nrel_val\n",
    "        else:\n",
    "            new_demand.at[idx, (year, \"diff %\")] = None\n",
    "\n",
    "new_demand = new_demand.round(2)\n",
    "\n",
    "def highlight_diff(val):\n",
    "    if pd.isna(val):\n",
    "        return ''\n",
    "    if val > 10:\n",
    "        return 'background-color: #ffcccc'  # rosso chiaro\n",
    "    elif val < -10:\n",
    "        return 'background-color: #cce5ff'  # blu chiaro\n",
    "    else:\n",
    "        return 'background-color: #d4edda'  # verde chiaro\n",
    "\n",
    "styled = new_demand.style\n",
    "\n",
    "for year in years:\n",
    "    styled = styled.applymap(highlight_diff, subset=pd.IndexSlice[:, (year, \"diff %\")])\n",
    "\n",
    "for year in years[:-1]:\n",
    "    styled = styled.set_properties(\n",
    "        subset=pd.IndexSlice[:, (year, \"diff %\")],\n",
    "        **{'border-right': '2px solid black'}\n",
    "    )\n",
    "\n",
    "styled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed electricity consumption per explicit industrial process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_processes = [\n",
    "    \"SMR CC\", \"Haber-Bosch\", \"ethanol from starch\", \"ethanol from starch CC\",\n",
    "    \"DRI\", \"DRI CC\", \"DRI H2\", \"BF-BOF\", \"BF-BOF CC\", \"EAF\",\n",
    "    \"dry clinker\", \"cement finishing\", \"dry clinker CC\"\n",
    "]\n",
    "\n",
    "for name, n in networks.items():\n",
    "    if \"sector\" not in name:\n",
    "        continue\n",
    "\n",
    "    link_stats = n.statistics.energy_balance(comps=[\"Link\"], aggregate_time=\"sum\")\n",
    "    df_filtered = link_stats[link_stats.index.get_level_values(\"carrier\").isin(target_processes)]\n",
    "\n",
    "    ac_input = {}\n",
    "\n",
    "    for proc in target_processes:\n",
    "        proc_df = df_filtered[df_filtered.index.get_level_values(\"carrier\") == proc]\n",
    "        inputs = proc_df[proc_df < 0].abs()\n",
    "        input_row = inputs.groupby(level=\"bus_carrier\").sum()\n",
    "        ac_val = input_row.get(\"AC\", 0)\n",
    "        ac_input[proc] = ac_val / 1e6  # convert to TWh\n",
    "\n",
    "    df_ac = pd.DataFrame.from_dict(ac_input, orient=\"index\", columns=[\"AC input [TWh]\"])\n",
    "    df_ac = df_ac.sort_values(by=\"AC input [TWh]\", ascending=False)\n",
    "\n",
    "    print(f\"\\n⚡ AC electricity consumption – network: {name}\")\n",
    "    print(df_ac.round(2).to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e-kerosene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, n in networks.items():\n",
    "    if \"sector\" in name:\n",
    "        kerosene_load = n.loads.query(\"carrier in 'kerosene for aviation'\").p_set.sum()\n",
    "        ekerosene_load = n.loads.query(\"carrier in 'e-kerosene for aviation'\").p_set.sum()\n",
    "\n",
    "n.loads.query(\"carrier in 'kerosene for aviation'\").p_set.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kerosene_load_all = {}\n",
    "\n",
    "for name, n in networks.items():\n",
    "    if \"sector\" in name:\n",
    "        load_stats = n.statistics.energy_balance(comps=[\"Load\"], aggregate_time=\"sum\")\n",
    "        kerosene_load = load_stats[\n",
    "            load_stats.index.get_level_values('carrier').str.contains('kerosene for aviation')\n",
    "        ].sum()  # somma totale per kerosene\n",
    "        ekerosene_load = load_stats[\n",
    "            load_stats.index.get_level_values('carrier').str.contains('e-kerosene for aviation')\n",
    "        ].sum()  # somma totale per e-kerosene\n",
    "\n",
    "        kerosene_load_all[name] = {\"kerosene\": kerosene_load, \"ekerosene\": ekerosene_load}\n",
    "\n",
    "# Stampa risultati\n",
    "for name, stats in kerosene_load_all.items():\n",
    "    print(f\"{name} - Kerosene Load: {stats['kerosene']} MWh, e-Kerosene Load: {stats['ekerosene']} MWh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sector analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_shape = \"data/demand_data/gadm41_USA_1.json\"\n",
    "distance_code = \"EPSG:3857\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carriers_of_interest = ['NH3', 'ethanol', 'DRI', 'steel BF-BOF', 'steel EAF', 'cement'] #'e-kerosene for aviation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_state_to_buses(network, path_shapes, distance_crs):\n",
    "    \"\"\"\n",
    "    Attach state to buses\n",
    "    \"\"\"\n",
    "    # Read the shapefile using geopandas\n",
    "    shapes = gpd.read_file(path_shapes, crs=distance_crs)\n",
    "    shapes[\"ISO_1\"] = shapes[\"ISO_1\"].apply(lambda x: x.split(\"-\")[1])\n",
    "    shapes.rename(columns={\"ISO_1\": \"State\"}, inplace=True)\n",
    "\n",
    "    ac_dc_carriers = [\"AC\", \"DC\"]\n",
    "    location_mapping = network.buses.query(\"carrier in @ac_dc_carriers\")[[\"x\", \"y\"]]\n",
    "\n",
    "    network.buses[\"x\"] = network.buses[\"location\"].map(location_mapping[\"x\"]).fillna(0)\n",
    "    network.buses[\"y\"] = network.buses[\"location\"].map(location_mapping[\"y\"]).fillna(0)\n",
    "    \n",
    "    pypsa_gpd = gpd.GeoDataFrame(\n",
    "            network.buses, \n",
    "            geometry=gpd.points_from_xy(network.buses.x, network.buses.y), \n",
    "            crs=4326\n",
    "        )\n",
    "\n",
    "    bus_cols = network.buses.columns\n",
    "    bus_cols = list(bus_cols) + [\"State\"]\n",
    "\n",
    "    st_buses = gpd.sjoin_nearest(shapes, pypsa_gpd, how=\"right\")[bus_cols]\n",
    "\n",
    "    network.buses[\"state\"] = st_buses[\"State\"]\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = gpd.read_file(path_shape).to_crs(\"EPSG:4326\")\n",
    "shapes[\"state\"] = shapes[\"ISO_1\"].str[-2:]  # \"US-CA\" → \"CA\"\n",
    "\n",
    "for name, net in networks.items():\n",
    "    if \"sector\" not in name.lower():\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing network: {name}\")\n",
    "\n",
    "    # 1. Match States to bus\n",
    "    net = attach_state_to_buses(net, path_shape, distance_code)\n",
    "\n",
    "    # 2. Filter load per carrier\n",
    "    filtered_loads = net.loads[net.loads[\"carrier\"].isin(carriers_of_interest)].copy()\n",
    "\n",
    "    # 3. Add State\n",
    "    filtered_loads[\"state\"] = filtered_loads[\"bus\"].map(net.buses[\"state\"])\n",
    "\n",
    "    # 4. Aggregate\n",
    "    load_by_state = (\n",
    "        filtered_loads.groupby([\"state\", \"carrier\"])[\"p_set\"]\n",
    "        .sum()\n",
    "        .mul(8760)  # MW → MWh\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    present_carriers = load_by_state[\"carrier\"].unique().tolist()\n",
    "    missing_carriers = [c for c in carriers_of_interest if c not in present_carriers]\n",
    "\n",
    "    # Aggiungi empty rows for missing carriers to make plotting nicer\n",
    "    for carrier in missing_carriers:\n",
    "        load_by_state = pd.concat([\n",
    "            load_by_state,\n",
    "            pd.DataFrame({\"state\": shapes[\"state\"], \"carrier\": carrier, \"p_set\": float(\"nan\")})\n",
    "        ], ignore_index=True)\n",
    "\n",
    "    # Order by carriers_of_interest\n",
    "    load_by_state[\"carrier\"] = pd.Categorical(load_by_state[\"carrier\"], categories=carriers_of_interest, ordered=True)\n",
    "    load_by_state = load_by_state.sort_values(\"carrier\")\n",
    "\n",
    "    n_carriers = len(carriers_of_interest)\n",
    "    n_cols = 2\n",
    "    n_rows = (n_carriers + n_cols - 1) // n_cols\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 8, n_rows * 5))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, carrier in enumerate(carriers_of_interest):\n",
    "        carrier_data = load_by_state[load_by_state[\"carrier\"] == carrier].copy()\n",
    "        map_df = shapes.merge(carrier_data, on=\"state\", how=\"left\")\n",
    "\n",
    "        ax = axes[i]\n",
    "        map_df.plot(\n",
    "            column=\"p_set\",\n",
    "            ax=ax,\n",
    "            legend=True,\n",
    "            cmap=\"OrRd\",\n",
    "            missing_kwds={\"color\": \"lightgrey\", \"label\": \"No data\"},\n",
    "            edgecolor=\"black\"\n",
    "        )\n",
    "        ax.set_title(f\"{carrier}\", fontsize=12)\n",
    "        ax.set_xlim([-180, -65])\n",
    "        ax.set_ylim([15,75])\n",
    "        ax.axis('off')\n",
    "\n",
    "        leg = ax.get_legend()\n",
    "        if leg:\n",
    "            leg.set_bbox_to_anchor((1, 0.5))\n",
    "            for t in leg.get_texts():\n",
    "                t.set_fontsize(8)\n",
    "\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    fig.suptitle(f\"Total Load by State [MWh] - {name}\", fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific conversion per carrier\n",
    "conversion_factors = {\n",
    "    \"cement\": (1 / 1000, \"kt/a\"),\n",
    "    \"DRI\": (1 / 1000, \"kt/a\"),\n",
    "    \"steel BF-BOF\": (1 / 1000, \"kt/a\"),\n",
    "    \"steel EAF\": (1 / 1000, \"kt/a\"),\n",
    "    \"ethanol\": (1/ 1000 / 5.1666, \"kt/a\"),  # 1 MWh = 5.1666 t\n",
    "    \"NH3\": (1/ 80.2 * 3600 / 1e6, \"Mgallon/a\"),  # 1 MWh = 80.2 gallon\n",
    "    # gli altri rimangono in MWh\n",
    "}\n",
    "\n",
    "summary_all = {}\n",
    "units = {}\n",
    "\n",
    "for name, network in networks.items():\n",
    "    if \"sector\" not in name.lower():\n",
    "        continue  \n",
    "\n",
    "    network = attach_state_to_buses(network, path_shape, distance_code)\n",
    "\n",
    "    filtered_loads = network.loads[network.loads[\"carrier\"].isin(carriers_of_interest)].copy()\n",
    "\n",
    "    filtered_loads[\"state\"] = filtered_loads[\"bus\"].map(network.buses[\"state\"])\n",
    "\n",
    "    summary = (\n",
    "        filtered_loads.groupby(\"carrier\")[\"p_set\"]\n",
    "        .sum()\n",
    "        .mul(8760)  # converti da MW a MWh/a\n",
    "        .reindex(carriers_of_interest)\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    converted = []\n",
    "    unit_column = []\n",
    "    for carrier in carriers_of_interest:\n",
    "        value = summary.get(carrier, 0)\n",
    "        factor, unit = conversion_factors.get(carrier, (1, \"MWh/a\"))\n",
    "        converted.append(round(value * factor, 2))\n",
    "        unit_column.append(unit)\n",
    "\n",
    "    summary_converted = pd.Series(converted, index=carriers_of_interest)\n",
    "    summary_all[name] = summary_converted\n",
    "\n",
    "    if \"Unit\" not in units:\n",
    "        units[\"Unit\"] = unit_column \n",
    "\n",
    "summary_df = pd.DataFrame(summary_all)\n",
    "summary_df[\"Unit\"] = units[\"Unit\"]\n",
    "summary_df.index.name = \"Carrier\"\n",
    "\n",
    "pd.set_option('display.float_format', '{:,.2f}'.format)\n",
    "\n",
    "print(\"Totale by carrier and network:\")\n",
    "print(summary_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_emissions_from_links(net):\n",
    "    import pandas as pd\n",
    "    import re\n",
    "\n",
    "    results = []\n",
    "\n",
    "    bus_cols = [col for col in net.links.columns if re.fullmatch(r\"bus\\d+\", col)]\n",
    "\n",
    "    for i, row in net.links.iterrows():\n",
    "        carrier = row[\"carrier\"]\n",
    "        link_name = i\n",
    "        co2_atmosphere = 0.0\n",
    "        co2_stored = 0.0\n",
    "\n",
    "        for j, bus_col in enumerate(bus_cols):\n",
    "            bus_val = str(row[bus_col]).lower().strip()\n",
    "            p_col = f\"p{j}\"\n",
    "\n",
    "            if p_col not in net.links_t or link_name not in net.links_t[p_col]:\n",
    "                continue\n",
    "\n",
    "            flow = net.links_t[p_col][link_name].mean() * 8760  # MWh/year\n",
    "\n",
    "            if \"co2 atmosphere\" in bus_val or \"co2 atmoshpere\" in bus_val:\n",
    "                co2_atmosphere -= flow\n",
    "            elif \"co2 stored\" in bus_val:\n",
    "                co2_stored += flow\n",
    "\n",
    "        results.append({\n",
    "            \"link\": link_name,\n",
    "            \"carrier\": carrier,\n",
    "            \"co2_atmosphere\": co2_atmosphere,\n",
    "            \"co2_stored\": co2_stored,\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df = df[(df[\"co2_atmosphere\"] != 0) | (df[\"co2_stored\"] != 0)]\n",
    "\n",
    "    # Group per carrier and convert in Mt\n",
    "    summary = df.groupby(\"carrier\")[[\"co2_atmosphere\", \"co2_stored\"]].sum().reset_index()\n",
    "    summary[\"net_emissions\"] = summary[\"co2_atmosphere\"] - summary[\"co2_stored\"]\n",
    "\n",
    "    summary[[\"co2_atmosphere\", \"co2_stored\", \"net_emissions\"]] = (\n",
    "        summary[[\"co2_atmosphere\", \"co2_stored\", \"net_emissions\"]] / 1e6\n",
    "    ).round(2)\n",
    "\n",
    "    summary = summary.rename(columns={\n",
    "        \"co2_atmosphere\": \"co2_atmosphere [Mt CO2]\",\n",
    "        \"co2_stored\": \"co2_stored [Mt CO2]\",\n",
    "        \"net_emissions\": \"net_emissions [Mt CO2]\"\n",
    "    })\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, net in networks.items():\n",
    "    if \"sector\" not in name:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "\n",
    "    df = compute_emissions_from_links(net)\n",
    "\n",
    "    # Filter rows with emissions != 0 only\n",
    "    df_nonzero = df[\n",
    "        (df[\"co2_atmosphere [Mt CO2]\"] != 0) |\n",
    "        (df[\"co2_stored [Mt CO2]\"] != 0) |\n",
    "        (df[\"net_emissions [Mt CO2]\"] != 0)\n",
    "    ]\n",
    "\n",
    "    if df_nonzero.empty:\n",
    "        print(\"No relevant emissions.\")\n",
    "    else:\n",
    "        print(df_nonzero.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_emissions_grouped(net, carrier_groups):\n",
    "    import pandas as pd\n",
    "    import re\n",
    "\n",
    "    results = []\n",
    "\n",
    "    bus_cols = [col for col in net.links.columns if re.fullmatch(r\"bus\\d+\", col)]\n",
    "\n",
    "    for i, row in net.links.iterrows():\n",
    "        carrier = row[\"carrier\"]\n",
    "        link_name = i\n",
    "        co2_atmosphere = 0.0\n",
    "        co2_stored = 0.0\n",
    "\n",
    "        for j, bus_col in enumerate(bus_cols):\n",
    "            bus_val = str(row[bus_col]).lower().strip()\n",
    "            p_col = f\"p{j}\"\n",
    "\n",
    "            if p_col not in net.links_t or link_name not in net.links_t[p_col]:\n",
    "                continue\n",
    "\n",
    "            flow = net.links_t[p_col][link_name].mean() * 8760  # MWh/year\n",
    "\n",
    "            if \"co2 atmosphere\" in bus_val or \"co2 atmoshpere\" in bus_val:\n",
    "                co2_atmosphere -= flow\n",
    "            elif \"co2 stored\" in bus_val:\n",
    "                co2_stored += flow\n",
    "\n",
    "        results.append({\n",
    "            \"link\": link_name,\n",
    "            \"carrier\": carrier,\n",
    "            \"co2_atmosphere\": co2_atmosphere,\n",
    "            \"co2_stored\": co2_stored,\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    all_grouped_carriers = set(sum(carrier_groups.values(), []))\n",
    "    df = df[df[\"carrier\"].isin(all_grouped_carriers)]\n",
    "\n",
    "    carrier_summary = df.groupby(\"carrier\")[[\"co2_atmosphere\", \"co2_stored\"]].sum().reset_index()\n",
    "\n",
    "    group_results = []\n",
    "    for group_name, group_carriers in carrier_groups.items():\n",
    "        group_df = carrier_summary[carrier_summary[\"carrier\"].isin(group_carriers)]\n",
    "        co2_atm = group_df[\"co2_atmosphere\"].sum()\n",
    "        co2_stored = group_df[\"co2_stored\"].sum()\n",
    "        net_emissions = co2_atm - co2_stored\n",
    "\n",
    "        group_results.append({\n",
    "            \"carrier_group\": group_name,\n",
    "            \"co2_atmosphere [Mt CO2]\": round(co2_atm / 1e6, 2),\n",
    "            \"co2_stored [Mt CO2]\": round(co2_stored / 1e6, 2),\n",
    "            \"net_emissions [Mt CO2]\": round(net_emissions / 1e6, 2)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(group_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carrier_groups = {\n",
    "    \"BF-BOF\": [\"BF-BOF\", \"BF-BOF CC\"],\n",
    "    \"DRI\": [\"DRI\", \"DRI CC\"],\n",
    "    \"ethanol from starch\": [\"ethanol from starch\", \"ethanol from starch CC\"],\n",
    "    \"SMR\": [\"SMR\", \"SMR CC\"],\n",
    "    \"dry clinker\": [\"dry clinker\", \"dry clinker CC\"]\n",
    "}\n",
    "\n",
    "for name, net in networks.items():\n",
    "    if \"sector\" not in name.lower():\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "\n",
    "    df_summary = compute_emissions_grouped(net, carrier_groups)\n",
    "\n",
    "    df_filtered = df_summary[\n",
    "        (df_summary[[\"co2_atmosphere [Mt CO2]\", \"co2_stored [Mt CO2]\", \"net_emissions [Mt CO2]\"]]\n",
    "         .abs().sum(axis=1) > 0)\n",
    "    ]\n",
    "\n",
    "    print(df_filtered.to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_emissions_by_state(net, carrier_groups):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    bus_cols = [col for col in net.links.columns if re.fullmatch(r\"bus\\d+\", col)]\n",
    "\n",
    "    for i, row in net.links.iterrows():\n",
    "        carrier = row[\"carrier\"]\n",
    "        link_name = i\n",
    "        co2_atmos = 0.0\n",
    "        co2_stored = 0.0\n",
    "\n",
    "        group = next((g for g, carriers in carrier_groups.items() if carrier in carriers), None)\n",
    "        if group is None:\n",
    "            continue\n",
    "\n",
    "        for j, bus_col in enumerate(bus_cols):\n",
    "            bus_val = str(row[bus_col]).lower().strip()\n",
    "            p_col = f\"p{j}\"\n",
    "\n",
    "            if p_col not in net.links_t or link_name not in net.links_t[p_col]:\n",
    "                continue\n",
    "\n",
    "            flow = net.links_t[p_col][link_name].mean() * 8760  # Mt CO2/year\n",
    "\n",
    "            if \"co2 atmosphere\" in bus_val or \"co2 atmoshpere\" in bus_val:\n",
    "                co2_atmos -= flow\n",
    "            elif \"co2 stored\" in bus_val:\n",
    "                co2_stored += flow\n",
    "\n",
    "        state = \"Unknown\"\n",
    "        for bus_col in bus_cols:\n",
    "            bus = row[bus_col]\n",
    "            if bus in net.buses.index:\n",
    "                s = net.buses.loc[bus, \"state\"]\n",
    "                if pd.notna(s) and s != \"Unknown\":\n",
    "                    state = s\n",
    "                    break\n",
    "\n",
    "        results.append({\n",
    "            \"state\": state,\n",
    "            \"group\": group,\n",
    "            \"co2_atmosphere\": co2_atmos,\n",
    "            \"co2_stored\": co2_stored\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    summary = df.groupby([\"state\", \"group\"])[[\"co2_atmosphere\", \"co2_stored\"]].sum().reset_index()\n",
    "    summary[\"net_emissions\"] = summary[\"co2_atmosphere\"] - summary[\"co2_stored\"]\n",
    "\n",
    "    summary[[\"co2_atmosphere\", \"co2_stored\", \"net_emissions\"]] = (\n",
    "        summary[[\"co2_atmosphere\", \"co2_stored\", \"net_emissions\"]] / 1e6 # Convert to MtCO2\n",
    "    ).round(2)\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_emissions_maps_by_group(all_state_emissions, path_shapes, distance_crs, title):\n",
    "\n",
    "    # Upload shapefile and force CRS\n",
    "    gdf_states = gpd.read_file(path_shapes).to_crs(\"EPSG:4326\")\n",
    "    gdf_states[\"State\"] = gdf_states[\"ISO_1\"].str[-2:]\n",
    "\n",
    "    groups = all_state_emissions[\"group\"].unique()\n",
    "    n = len(groups)\n",
    "    ncols = 2\n",
    "    nrows = int(np.ceil(n / ncols))\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(8 * ncols, 8 * nrows))\n",
    "    axes = axes.flat if n > 1 else [axes]\n",
    "\n",
    "    for i, group in enumerate(groups):\n",
    "        ax = axes[i]\n",
    "        df_group = all_state_emissions[all_state_emissions[\"group\"] == group].copy()\n",
    "        df_group = df_group.rename(columns={\"State\": \"State\"})\n",
    "\n",
    "        merged = gdf_states.merge(df_group, on=\"State\", how=\"left\")\n",
    "        \n",
    "        merged.plot(\n",
    "            column=\"net_emissions\",\n",
    "            cmap=\"Reds\",\n",
    "            legend=True,\n",
    "            ax=ax,\n",
    "            missing_kwds={\"color\": \"lightgrey\", \"label\": \"No data\"},\n",
    "            edgecolor=\"black\"\n",
    "        )\n",
    "        \n",
    "        ax.set_title(f\"{group}\", fontsize=12)\n",
    "        ax.set_xlim([-180, -65])\n",
    "        ax.set_ylim([15, 75])\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        leg = ax.get_legend()\n",
    "        if leg:\n",
    "            leg.set_bbox_to_anchor((1, 0.5))\n",
    "            for t in leg.get_texts():\n",
    "                t.set_fontsize(8)\n",
    "\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    fig.suptitle(f\"Net Emissions by process and State [MtCO2/yr] - {title}\", fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_states = gpd.read_file(path_shape)\n",
    "gdf_states = gdf_states.to_crs(\"EPSG:4326\") \n",
    "\n",
    "for name, net in networks.items():\n",
    "    if \"sector\" not in name.lower():\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "\n",
    "    df_state_emissions = compute_emissions_by_state(net, carrier_groups)\n",
    "\n",
    "    df_filtered = df_filtered.rename(columns={\"state\": \"State\"})\n",
    "\n",
    "    print(df_filtered.rename(columns={\n",
    "        \"co2_atmosphere\": \"CO2 to Atmosphere [MtCO2/yr]\",\n",
    "        \"co2_stored\": \"CO2 Stored [MtCO2/yr]\",\n",
    "        \"net_emissions\": \"Net Emissions [MtCO2/yr]\"\n",
    "    }).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, net in networks.items():\n",
    "    if \"sector\" not in name.lower():\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "\n",
    "    df_state_emissions = compute_emissions_by_state(net, carrier_groups)\n",
    "    df_filtered = df_state_emissions.rename(columns={\"state\": \"State\"})\n",
    "\n",
    "    plot_emissions_maps_by_group(df_filtered, path_shape, distance_code, name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
